{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asu-trans-ai-lab/GTFS2GMNS/blob/main/gtfs2gmns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cCSGNLJf3z7"
      },
      "source": [
        "##Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ghEaf-d2Fdm"
      },
      "source": [
        "**Load the GTFS file from the repository of GTFS testing datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3xZMKsJ2Ew2",
        "outputId": "7834c74f-a3cf-4105-cf37-349fe5594651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'GTFS2GMNS'...\n",
            "remote: Enumerating objects: 522, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 522 (delta 48), reused 51 (delta 33), pack-reused 439 (from 1)\u001b[K\n",
            "Receiving objects: 100% (522/522), 306.19 MiB | 21.67 MiB/s, done.\n",
            "Resolving deltas: 100% (171/171), done.\n",
            "[Errno 2] No such file or directory: 'GTFS2GMNS/test/GTFS'\n",
            "/Users/willicon/Desktop/roanoke_benchmark/GTFS2GMNS\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ./GTFS2GMNS/\n",
        "!git clone https://github.com/asu-trans-ai-lab/GTFS2GMNS\n",
        "\n",
        "%cd GTFS2GMNS/test/GTFS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CexwTGDB0D0A"
      },
      "source": [
        "Check the file icon on the left hand side, makesure files stops.txt, routes.txt, trips.txt, stop_times.txt, agency.txt exist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6y7B8WX-d46"
      },
      "source": [
        "**Import python packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mysg2UEz0cu5"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import math\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbceNHC5gHXR"
      },
      "source": [
        "##Convert GTFS to GMNS Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfHTwVuXhVHf",
        "outputId": "b71692a5-23cd-451d-f8e4-69c6949ca308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start converting Agency_1...\n",
            "Directory : /Users/willicon/Desktop/roanoke_benchmark/gtfs\n",
            "start reading input GTFS files...\n",
            "agent_name: Valley Metro Roanoke\n",
            "number of stops = 834\n",
            "number of routes = 33\n",
            "number of trips = 592 ... 592\n",
            "number of stop_time records = 14648\n",
            "number of stop_time records after dropping empty arrival and departure time = 14648\n",
            "start converting the time stamps...\n",
            "start marking terminal flags for stops...\n",
            "add terminal_flag for trips using CPU time: 1.4153788089752197 s\n",
            "concatenate different trips using CPU time: 0.05683493614196777 s\n",
            "have updated 14648 stop_time records\n",
            "merge the route information with trip information...\n",
            "number of final merged records = 14648\n",
            "Data reading done..\n",
            "node.csv of /Users/willicon/Desktop/roanoke_benchmark/gtfs has been generated...\n",
            "1. start creating route links...\n",
            "convert  50 service links successfully... using time 0.09826827049255371 s\n",
            "convert  100 service links successfully... using time 0.16457915306091309 s\n",
            "convert  150 service links successfully... using time 0.20791411399841309 s\n",
            "convert  200 service links successfully... using time 0.24935507774353027 s\n",
            "convert  250 service links successfully... using time 0.2948741912841797 s\n",
            "convert  300 service links successfully... using time 0.34140515327453613 s\n",
            "convert  350 service links successfully... using time 0.3946082592010498 s\n",
            "convert  400 service links successfully... using time 0.4425232410430908 s\n",
            "convert  450 service links successfully... using time 0.48938512802124023 s\n",
            "convert  500 service links successfully... using time 0.5395991802215576 s\n",
            "convert  550 service links successfully... using time 0.5839071273803711 s\n",
            "convert  600 service links successfully... using time 0.639747142791748 s\n",
            "convert  650 service links successfully... using time 0.6862661838531494 s\n",
            "convert  700 service links successfully... using time 0.7333250045776367 s\n",
            "convert  750 service links successfully... using time 0.776648998260498 s\n",
            "convert  800 service links successfully... using time 0.8217551708221436 s\n",
            "convert  850 service links successfully... using time 0.8759932518005371 s\n",
            "convert  900 service links successfully... using time 0.9193952083587646 s\n",
            "convert  950 service links successfully... using time 0.9662511348724365 s\n",
            "convert  1000 service links successfully... using time 1.0052239894866943 s\n",
            "convert  1050 service links successfully... using time 1.0454909801483154 s\n",
            "2. start creating boarding links from stations to their passing routes...\n",
            "convert  50 boarding links successfully... using time 1.09104323387146 s\n",
            "convert  100 boarding links successfully... using time 1.0938060283660889 s\n",
            "convert  150 boarding links successfully... using time 1.0979650020599365 s\n",
            "convert  200 boarding links successfully... using time 1.1006391048431396 s\n",
            "convert  250 boarding links successfully... using time 1.1040189266204834 s\n",
            "convert  300 boarding links successfully... using time 1.1085140705108643 s\n",
            "convert  350 boarding links successfully... using time 1.1114051342010498 s\n",
            "convert  400 boarding links successfully... using time 1.11549711227417 s\n",
            "convert  450 boarding links successfully... using time 1.1197450160980225 s\n",
            "convert  500 boarding links successfully... using time 1.1234641075134277 s\n",
            "convert  550 boarding links successfully... using time 1.1272649765014648 s\n",
            "convert  600 boarding links successfully... using time 1.1305861473083496 s\n",
            "convert  650 boarding links successfully... using time 1.1341602802276611 s\n",
            "convert  700 boarding links successfully... using time 1.1368441581726074 s\n",
            "convert  750 boarding links successfully... using time 1.1404011249542236 s\n",
            "convert  800 boarding links successfully... using time 1.143557071685791 s\n",
            "convert  850 boarding links successfully... using time 1.146665096282959 s\n",
            "convert  900 boarding links successfully... using time 1.1499741077423096 s\n",
            "convert  950 boarding links successfully... using time 1.1548840999603271 s\n",
            "convert  1000 boarding links successfully... using time 1.1575322151184082 s\n",
            "convert  1050 boarding links successfully... using time 1.1605591773986816 s\n",
            "convert  1100 boarding links successfully... using time 1.1633241176605225 s\n",
            "convert  1150 boarding links successfully... using time 1.1664631366729736 s\n",
            "convert  1200 boarding links successfully... using time 1.1729331016540527 s\n",
            "convert  1250 boarding links successfully... using time 1.1814730167388916 s\n",
            "convert  1300 boarding links successfully... using time 1.1845650672912598 s\n",
            "convert  1350 boarding links successfully... using time 1.1890511512756348 s\n",
            "convert  1400 boarding links successfully... using time 1.1924850940704346 s\n",
            "convert  1450 boarding links successfully... using time 1.1950981616973877 s\n",
            "convert  1500 boarding links successfully... using time 1.1971211433410645 s\n",
            "convert  1550 boarding links successfully... using time 1.198923110961914 s\n",
            "convert  1600 boarding links successfully... using time 1.2017359733581543 s\n",
            "convert  1650 boarding links successfully... using time 1.2053701877593994 s\n",
            "convert  1700 boarding links successfully... using time 1.2088849544525146 s\n",
            "convert  1750 boarding links successfully... using time 1.210752010345459 s\n",
            "convert  1800 boarding links successfully... using time 1.2144219875335693 s\n",
            "convert  1850 boarding links successfully... using time 1.2159271240234375 s\n",
            "convert  1900 boarding links successfully... using time 1.2187221050262451 s\n",
            "convert  1950 boarding links successfully... using time 1.2212560176849365 s\n",
            "convert  2000 boarding links successfully... using time 1.2236852645874023 s\n",
            "convert  2050 boarding links successfully... using time 1.2273023128509521 s\n",
            "convert  2100 boarding links successfully... using time 1.2313122749328613 s\n",
            "convert  2150 boarding links successfully... using time 1.2337391376495361 s\n",
            "convert  2200 boarding links successfully... using time 1.2350091934204102 s\n",
            "Conversion of  Agency2...  have done..\n",
            "convert  50 transferring links successfully... using time 0.1391899585723877 s\n",
            "convert  100 transferring links successfully... using time 0.20901918411254883 s\n",
            "convert  150 transferring links successfully... using time 0.28208494186401367 s\n",
            "convert  200 transferring links successfully... using time 0.352064847946167 s\n",
            "convert  250 transferring links successfully... using time 0.4382331371307373 s\n",
            "convert  300 transferring links successfully... using time 0.5222530364990234 s\n",
            "convert  350 transferring links successfully... using time 0.5888528823852539 s\n",
            "convert  400 transferring links successfully... using time 0.6767678260803223 s\n",
            "convert  450 transferring links successfully... using time 0.7297189235687256 s\n",
            "convert  500 transferring links successfully... using time 0.7883539199829102 s\n",
            "convert  550 transferring links successfully... using time 0.87381911277771 s\n",
            "convert  600 transferring links successfully... using time 0.9534041881561279 s\n",
            "convert  650 transferring links successfully... using time 1.0634269714355469 s\n",
            "convert  700 transferring links successfully... using time 1.1487772464752197 s\n",
            "convert  750 transferring links successfully... using time 1.23073410987854 s\n",
            "convert  800 transferring links successfully... using time 1.3057730197906494 s\n",
            "convert  850 transferring links successfully... using time 1.3650619983673096 s\n",
            "convert  900 transferring links successfully... using time 1.4277281761169434 s\n",
            "convert  950 transferring links successfully... using time 1.5174219608306885 s\n",
            "convert  1000 transferring links successfully... using time 1.6183972358703613 s\n",
            "convert  1050 transferring links successfully... using time 1.6894590854644775 s\n",
            "convert  1100 transferring links successfully... using time 1.7608098983764648 s\n",
            "convert  1150 transferring links successfully... using time 1.8360919952392578 s\n",
            "convert  1200 transferring links successfully... using time 1.9152710437774658 s\n",
            "convert  1250 transferring links successfully... using time 1.9881939888000488 s\n",
            "convert  1300 transferring links successfully... using time 2.0437710285186768 s\n",
            "convert  1350 transferring links successfully... using time 2.093317985534668 s\n",
            "convert  1400 transferring links successfully... using time 2.1572389602661133 s\n",
            "convert  1450 transferring links successfully... using time 2.281667947769165 s\n",
            "convert  1500 transferring links successfully... using time 2.4610610008239746 s\n",
            "convert  1550 transferring links successfully... using time 2.523369073867798 s\n",
            "convert  1600 transferring links successfully... using time 2.6056301593780518 s\n",
            "convert  1650 transferring links successfully... using time 2.676339864730835 s\n",
            "convert  1700 transferring links successfully... using time 2.743417978286743 s\n",
            "convert  1750 transferring links successfully... using time 2.819020986557007 s\n",
            "convert  1800 transferring links successfully... using time 2.877610921859741 s\n",
            "convert  1850 transferring links successfully... using time 2.9356729984283447 s\n",
            "convert  1900 transferring links successfully... using time 3.025285005569458 s\n",
            "convert  1950 transferring links successfully... using time 3.0959038734436035 s\n",
            "convert  2000 transferring links successfully... using time 3.1766040325164795 s\n",
            "convert  2050 transferring links successfully... using time 3.215575933456421 s\n",
            "convert  2100 transferring links successfully... using time 3.2869648933410645 s\n",
            "convert  2150 transferring links successfully... using time 3.3440911769866943 s\n",
            "convert  2200 transferring links successfully... using time 3.4179911613464355 s\n",
            "convert  2250 transferring links successfully... using time 3.477466106414795 s\n",
            "convert  2300 transferring links successfully... using time 3.577162027359009 s\n",
            "convert  2350 transferring links successfully... using time 3.6995949745178223 s\n",
            "convert  2400 transferring links successfully... using time 3.8134031295776367 s\n",
            "convert  2450 transferring links successfully... using time 3.8887851238250732 s\n",
            "convert  2500 transferring links successfully... using time 3.9296722412109375 s\n",
            "convert  2550 transferring links successfully... using time 3.9745140075683594 s\n",
            "convert  2600 transferring links successfully... using time 4.026456117630005 s\n",
            "convert  2650 transferring links successfully... using time 4.059837102890015 s\n",
            "convert  2700 transferring links successfully... using time 4.092778921127319 s\n",
            "convert  2750 transferring links successfully... using time 4.130632162094116 s\n",
            "convert  2800 transferring links successfully... using time 4.16191291809082 s\n",
            "convert  2850 transferring links successfully... using time 4.198491096496582 s\n",
            "convert  2900 transferring links successfully... using time 4.237519025802612 s\n",
            "run time --> 7.477490186691284\n"
          ]
        }
      ],
      "source": [
        "def reading_data(gtfs_path):\n",
        "    print('start reading input GTFS files...')\n",
        "    agency_df = _reading_text(gtfs_path + os.sep + 'agency')\n",
        "    agency_name = agency_df['agency_name'][0]\n",
        "    if '\"' in agency_name:\n",
        "        agency_name = eval(agency_name)\n",
        "        #  Remove quotes from string, for example '\"Arlington Transit\"' --> 'Arlington Transit'\n",
        "    print(\"agent_name:\", agency_name)\n",
        "\n",
        "    stop_df = _reading_text(gtfs_path + os.sep + 'stops')\n",
        "    stop_df = stop_df[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
        "    print(\"number of stops =\", len(stop_df))\n",
        "    #  select only latitude and longitude of the stops/stations\n",
        "\n",
        "    route_df = _reading_text(gtfs_path + os.sep + 'routes')\n",
        "    route_df = route_df[['route_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
        "    print(\"number of routes =\", len(route_df))\n",
        "\n",
        "    trip_df = _reading_text(gtfs_path + os.sep + 'trips')\n",
        "   \n",
        "    if 'direction_id' not in trip_df.columns.tolist():  # direction_id is mandatory field name here\n",
        "        trip_df['direction_id'] = str(0)\n",
        "   \n",
        "    # Deal with special issues of direction_id exists but all values are NaN\n",
        "    try:\n",
        "        trip_df['direction_id'] = trip_df.apply(lambda x: str(2 - int(x['direction_id'])), axis=1)\n",
        "    except Exception:\n",
        "        trip_df['direction_id'] = \"0\"\n",
        "\n",
        "    directed_route_id = trip_df['route_id'].astype(str).str.cat(\n",
        "        trip_df['direction_id'].astype(str), sep='.')\n",
        "    trip_df['directed_route_id'] = directed_route_id  # add a new column \"directed_route_id\"\n",
        "    #  If trips on a route service opposite directions,distinguish directions using values 0 and 1.\n",
        "    # revise the direction_id from 0,1 to 2,1\n",
        "    # add a new field directed_route_id\n",
        "    # deal with special issues of Agency 12 Fairfax CUE # Alicia, Nov 10:\n",
        "    # route file has route id with quotes, e.g., '\"green2\"' while trip file does not have it, e.g.,'green2'\n",
        "    if (route_df['route_id'][0][0] == '\"') != (trip_df['route_id'][0][0] == '\"'):\n",
        "        if route_df['route_id'][0][0] == '\"':\n",
        "            route_df['route_id'] = route_df.apply(lambda x: x['route_id'].strip('\"'), axis=1)\n",
        "        else:\n",
        "            trip_df['route_id'] = trip_df.apply(lambda x: x['route_id'].strip('\"'), axis=1)\n",
        "    trip_route_df = pd.merge(trip_df, route_df, on='route_id')  # might go wrong in Agency 12\n",
        "    print(\"number of trips =\", len(trip_route_df), \"...\", len(trip_df))\n",
        "    #  as route is higher level planning than trips, len(trip_route_df)=len(trip_df)\n",
        "\n",
        "    stop_time_df = _reading_text(gtfs_path + os.sep + 'stop_times')\n",
        "    print(\"number of stop_time records =\", len(stop_time_df))\n",
        "    # drop the stations without accurate arrival and departure time.\n",
        "\n",
        "    # drop nan\n",
        "    stop_time_df = stop_time_df.dropna(subset=['arrival_time'], how='any')\n",
        "    # drop ''\n",
        "    stop_time_df = stop_time_df[stop_time_df.arrival_time != '']\n",
        "    stop_time_df = stop_time_df[stop_time_df.departure_time != '']\n",
        "\n",
        "    # drop ' '\n",
        "    stop_time_df = stop_time_df[stop_time_df.arrival_time != ' ']\n",
        "    stop_time_df = stop_time_df[stop_time_df.departure_time != ' ']\n",
        "    print(\"number of stop_time records after dropping empty arrival and departure time =\", len(stop_time_df))\n",
        "\n",
        "    # convert timestamp to minute\n",
        "    # as some agencies might have trips overlapping two days, should use _to_timedelta to convert the data\n",
        "    print(\"start converting the time stamps...\")\n",
        "    tt = datetime.datetime(2021, 1, 1, 0, 0, 0, 0)\n",
        "    stop_time_df['arrival_time'] = pd.to_timedelta(stop_time_df['arrival_time']) + tt\n",
        "    stop_time_df['departure_time'] = pd.to_timedelta(stop_time_df['departure_time']) + tt\n",
        "    stop_time_df['arrival_time'] = \\\n",
        "        stop_time_df['arrival_time'].apply(lambda x: x.hour * 60 + x.minute + 1440 * (x.day - 1))\n",
        "    stop_time_df['departure_time'] = \\\n",
        "        stop_time_df['departure_time'].apply(lambda x: x.hour * 60 + x.minute + 1440 * (x.day - 1))\n",
        "\n",
        "    print(\"start marking terminal flags for stops...\")\n",
        "    iteration_group = stop_time_df.groupby(['trip_id'])\n",
        "    # mark terminal flag for each stop. The terminals can only be determined at the level of trips\n",
        "    input_list = []\n",
        "    time_start = time.time()\n",
        "    for trip_id, trip_stop_time_df in iteration_group:\n",
        "        trip_stop_time_df = trip_stop_time_df.sort_values(by=['stop_sequence'])\n",
        "        trip_stop_time_df = trip_stop_time_df.reset_index()\n",
        "        # select only the trips within the provided time window\n",
        "        if (trip_stop_time_df.arrival_time.min() <= period_end_time) & (\n",
        "                trip_stop_time_df.arrival_time.min() >= period_start_time):\n",
        "            input_list.append(trip_stop_time_df)\n",
        "    intermediate_output_list = list(map(_determine_terminal_flag, input_list))\n",
        "    output_list = list(map(_stop_sequence_label, intermediate_output_list))\n",
        "\n",
        "    # use map function to speed up marking process\n",
        "    time_end = time.time()\n",
        "    print('add terminal_flag for trips using CPU time:', time_end - time_start, 's')\n",
        "\n",
        "    time_start = time.time()\n",
        "    stop_time_df_with_terminal = pd.concat(output_list, axis=0)\n",
        "    # concatenating a list is much faster than concatenating separate dataframes\n",
        "    time_end = time.time()\n",
        "    print('concatenate different trips using CPU time:', time_end - time_start, 's')\n",
        "    print(\"have updated\", len(stop_time_df_with_terminal), \"stop_time records\")\n",
        "    print(\"merge the route information with trip information...\")\n",
        "    directed_trip_route_stop_time_df = pd.merge(trip_route_df, stop_time_df_with_terminal, on='trip_id')\n",
        "    print(\"number of final merged records =\", len(directed_trip_route_stop_time_df))\n",
        "    print(\"Data reading done..\")\n",
        "\n",
        "    #  as trip is higher level planning than stop time scheduling, len(stop_time_df)>=len(trip_df)\n",
        "    #  Each record of directed_trip_route_stop_time_df represents a space-time state of a vehicle\n",
        "    # trip_id (different vehicles, e.g., train lines)\n",
        "    # stop_id (spatial location of the vehicle)\n",
        "    # arrival_time,departure_time (time index of the vehicle)\n",
        "\n",
        "    directed_route_stop_id = directed_trip_route_stop_time_df['directed_route_id'].astype(\n",
        "        str).str.cat(directed_trip_route_stop_time_df['stop_id'].astype(str), sep='.')\n",
        "    directed_trip_route_stop_time_df['directed_route_stop_id'] = directed_route_stop_id\n",
        "    #  directed_route_stop_id is a unique id to identify the route, direction, and stop of a vehicle at a time point\n",
        "    directed_trip_route_stop_time_df['stop_sequence'] \\\n",
        "        = directed_trip_route_stop_time_df['stop_sequence'].astype('int32')\n",
        "    # two important concepts : 1 directed_service_stop_id (directed_route_stop_id + stop sequence)\n",
        "    directed_trip_route_stop_time_df['directed_service_stop_id'] = \\\n",
        "        directed_trip_route_stop_time_df.directed_route_stop_id.astype(str) + ':' + \\\n",
        "        directed_trip_route_stop_time_df.stop_sequence_label\n",
        "    # 2. directed service id (directed_route_id + stop sequence) same directed route id might have different sequences\n",
        "    directed_trip_route_stop_time_df['directed_service_id'] = \\\n",
        "        directed_trip_route_stop_time_df.directed_route_id.astype(str) + ':' + \\\n",
        "        directed_trip_route_stop_time_df.stop_sequence_label\n",
        "    #  attach stop name and geometry for stops\n",
        "    directed_trip_route_stop_time_df = pd.merge(directed_trip_route_stop_time_df, stop_df, on='stop_id')\n",
        "    directed_trip_route_stop_time_df['agency_name'] = agency_name\n",
        "\n",
        "    return stop_df, route_df, trip_df, trip_route_df, stop_time_df, directed_trip_route_stop_time_df\n",
        "\n",
        "\n",
        "def create_nodes(directed_trip_route_stop_time_df, agency_num):\n",
        "    \"\"\"create physical (station) node...\"\"\"\n",
        "    physical_node_df = pd.DataFrame()\n",
        "    temp_df = directed_trip_route_stop_time_df.drop_duplicates(subset=['stop_id'])\n",
        "    physical_node_df['name'] = temp_df['stop_id']\n",
        "    physical_node_df = physical_node_df.sort_values(by=['name'])\n",
        "    physical_node_df['node_id'] = \\\n",
        "        np.linspace(start=1, stop=len(physical_node_df), num=len(physical_node_df)).astype('int32')\n",
        "    physical_node_df['node_id'] += int('{}000000'.format(agency_num))\n",
        "    physical_node_df['physical_node_id'] = physical_node_df['node_id']\n",
        "    physical_node_df['x_coord'] = temp_df['stop_lon'].astype(float)\n",
        "    physical_node_df['y_coord'] = temp_df['stop_lat'].astype(float)\n",
        "    physical_node_df['route_type'] = temp_df['route_type']\n",
        "    physical_node_df['route_id'] = temp_df['route_id']\n",
        "    physical_node_df['node_type'] = \\\n",
        "        physical_node_df.apply(lambda x: _convert_route_type_to_node_type_p(x.route_type), axis=1)\n",
        "    physical_node_df['directed_route_id'] = \"\"\n",
        "    physical_node_df['directed_service_id'] = \"\"\n",
        "    physical_node_df['zone_id'] = \"\"\n",
        "    physical_node_df['agency_name'] = temp_df['agency_name']\n",
        "    physical_node_df['geometry'] = 'POINT (' + physical_node_df['x_coord'].astype(str) + \\\n",
        "                                   ' ' + physical_node_df['y_coord'].astype(str) + ')'\n",
        "    stop_name_id_dict = dict(zip(physical_node_df['name'], physical_node_df['node_id']))\n",
        "    physical_node_df['terminal_flag'] = temp_df['terminal_flag']\n",
        "    physical_node_df['ctrl_type'] = \"\"\n",
        "    physical_node_df['agent_type'] = \"\"\n",
        "\n",
        "    \"\"\" create service node...\"\"\"\n",
        "    service_node_df = pd.DataFrame()\n",
        "    temp_df = directed_trip_route_stop_time_df.drop_duplicates(subset=['directed_service_stop_id'])\n",
        "    # 2.2.2 route stop node\n",
        "    service_node_df['name'] = temp_df['directed_service_stop_id']\n",
        "    service_node_df = service_node_df.sort_values(by=['name'])\n",
        "    service_node_df['node_id'] = \\\n",
        "        np.linspace(start=1, stop=len(service_node_df), num=len(service_node_df)).astype('int32')\n",
        "    service_node_df['physical_node_id'] = temp_df.apply(lambda x: stop_name_id_dict[x.stop_id], axis=1)\n",
        "    service_node_df['node_id'] += int('{}500000'.format(agency_num))\n",
        "\n",
        "    service_node_df['x_coord'] = temp_df['stop_lon'].astype(float) - 0.000100\n",
        "    service_node_df['y_coord'] = temp_df['stop_lat'].astype(float) - 0.000100\n",
        "    service_node_df['route_type'] = temp_df['route_type']\n",
        "    service_node_df['route_id'] = temp_df['route_id']\n",
        "    service_node_df['node_type'] = \\\n",
        "        service_node_df.apply(lambda x: _convert_route_type_to_node_type_s(x.route_type), axis=1)\n",
        "    # node_csv['terminal_flag'] = ' '\n",
        "    service_node_df['directed_route_id'] = temp_df['directed_route_id'].astype(str)\n",
        "    service_node_df['directed_service_id'] = temp_df['directed_service_id'].astype(str)\n",
        "    service_node_df['zone_id'] = \"\"\n",
        "    service_node_df['agency_name'] = temp_df['agency_name']\n",
        "    service_node_df['geometry'] = \\\n",
        "        'POINT (' + service_node_df['x_coord'].astype(str) + ' ' + service_node_df['y_coord'].astype(str) + ')'\n",
        "\n",
        "    service_node_df['terminal_flag'] = temp_df['terminal_flag']\n",
        "    service_node_df['ctrl_type'] = \"\"\n",
        "    service_node_df['agent_type'] = \"\"\n",
        "    # concatenate service and physical node\n",
        "    node_df = pd.concat([physical_node_df, service_node_df])\n",
        "    return node_df\n",
        "\n",
        "\n",
        "def create_service_boarding_links(directed_trip_route_stop_time_df, node_df, agency_num, one_agency_link_list):\n",
        "    \"\"\"dictionaries\"\"\"\n",
        "    node_id_dict = dict(zip(node_df['name'], node_df['node_id']))\n",
        "    directed_service_dict = dict(zip(node_df['node_id'], node_df['name']))\n",
        "    node_lon_dict = dict(zip(node_df['node_id'], node_df['x_coord']))\n",
        "    node_lat_dict = dict(zip(node_df['node_id'], node_df['y_coord']))\n",
        "    frequency_dict = {}\n",
        "\n",
        "    print(\"1. start creating route links...\")\n",
        "    \"\"\"service links\"\"\"\n",
        "    number_of_route_links = 0\n",
        "    iteration_group = directed_trip_route_stop_time_df.groupby('directed_service_id')\n",
        "    labeled_directed_service_list = []\n",
        "\n",
        "    time_start = time.time()\n",
        "    for directed_service_id, route_df in iteration_group:\n",
        "        if directed_service_id in labeled_directed_service_list:\n",
        "            continue\n",
        "        else:\n",
        "            labeled_directed_service_list.append(directed_service_id)\n",
        "            number_of_trips = len(route_df.trip_id.unique())\n",
        "            frequency_dict[directed_service_id] = number_of_trips  # note the frequency of routes\n",
        "            one_line_df = route_df[route_df.trip_id == route_df.trip_id.unique()[0]]\n",
        "            one_line_df = one_line_df.sort_values(by=['stop_sequence'])\n",
        "            number_of_records = len(one_line_df)\n",
        "            one_line_df = one_line_df.reset_index()\n",
        "            for k in range(number_of_records - 1):\n",
        "                link_id = 1000000 * agency_num + number_of_route_links + 1\n",
        "                from_node_id = node_id_dict[one_line_df.iloc[k].directed_service_stop_id]\n",
        "                to_node_id = node_id_dict[one_line_df.iloc[k + 1].directed_service_stop_id]\n",
        "                facility_type = _convert_route_type_to_link_type(one_line_df.iloc[k].route_type)\n",
        "                dir_flag = 1\n",
        "                directed_route_id = one_line_df.iloc[k].directed_route_id\n",
        "                link_type = 1\n",
        "                link_type_name = 'service_links'\n",
        "                from_node_lon = float(one_line_df.iloc[k].stop_lon)\n",
        "                from_node_lat = float(one_line_df.iloc[k].stop_lat)\n",
        "                to_node_lon = float(one_line_df.iloc[k + 1].stop_lon)\n",
        "                to_node_lat = float(one_line_df.iloc[k + 1].stop_lat)\n",
        "                length = _calculate_distance_from_geometry(from_node_lon, from_node_lat, to_node_lon, to_node_lat)\n",
        "                lanes = number_of_trips\n",
        "                capacity = 999999\n",
        "                VDF_fftt1 = one_line_df.iloc[k + 1].arrival_time - one_line_df.iloc[k].arrival_time\n",
        "                # minutes\n",
        "                VDF_cap1 = lanes * capacity\n",
        "                free_speed = ((length / 1000) / (VDF_fftt1 + 0.001)) * 60\n",
        "                # (kilometers/minutes)*60 = kilometer/hour\n",
        "                VDF_alpha1 = 0.15\n",
        "                VDF_beta1 = 4\n",
        "                VDF_penalty1 = 0\n",
        "                cost = 0\n",
        "                geometry = 'LINESTRING (' + str(from_node_lon) + ' ' + str(from_node_lat) + ', ' + \\\n",
        "                           str(to_node_lon) + ' ' + str(to_node_lat) + ')'\n",
        "                agency_name = one_line_df.agency_name[0]\n",
        "                allowed_use = _allowed_use_function(one_line_df.iloc[k].route_type)\n",
        "                stop_sequence = one_line_df.iloc[k].stop_sequence\n",
        "                directed_service_id = one_line_df.iloc[k].directed_service_id\n",
        "                link_list = [link_id, from_node_id, to_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                             link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                             VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use,\n",
        "                             agency_name,\n",
        "                             stop_sequence, directed_service_id]\n",
        "                one_agency_link_list.append(link_list)\n",
        "                number_of_route_links += 1\n",
        "                if number_of_route_links % 50 == 0:\n",
        "                    time_end = time.time()\n",
        "                    print('convert ', number_of_route_links,\n",
        "                          'service links successfully...', 'using time', time_end - time_start, 's')\n",
        "\n",
        "    print(\"2. start creating boarding links from stations to their passing routes...\")\n",
        "    \"\"\"boarding_links\"\"\"\n",
        "    service_node_df = node_df[node_df.node_id != node_df.physical_node_id]\n",
        "    #  select service node from node_df\n",
        "    service_node_df = service_node_df.reset_index()\n",
        "    number_of_sta2route_links = 0\n",
        "    for iter, row in service_node_df.iterrows():\n",
        "        link_id = agency_num * 1000000 + number_of_route_links + number_of_sta2route_links\n",
        "        from_node_id = row.physical_node_id\n",
        "        to_node_id = row.node_id\n",
        "        facility_type = _convert_route_type_to_link_type(row.route_type)\n",
        "        dir_flag = 1\n",
        "        directed_route_id = row.directed_route_id\n",
        "        link_type = 2\n",
        "        link_type_name = 'boarding_links'\n",
        "        to_node_lon = row.x_coord\n",
        "        to_node_lat = row.y_coord\n",
        "        from_node_lon = node_lon_dict[row.physical_node_id]\n",
        "        from_node_lat = node_lat_dict[row.physical_node_id]\n",
        "        length = _calculate_distance_from_geometry(from_node_lon, from_node_lat, to_node_lon, to_node_lat)\n",
        "        free_speed = 2\n",
        "        lanes = 1\n",
        "        capacity = 999999\n",
        "        VDF_cap1 = lanes * capacity\n",
        "        VDF_alpha1 = 0.15\n",
        "        VDF_beta1 = 4\n",
        "        VDF_penalty1 = 0\n",
        "        cost = 0\n",
        "        stop_sequence = -1\n",
        "        directed_service_id = directed_service_dict[to_node_id]\n",
        "        geometry = 'LINESTRING (' + str(from_node_lon) + ' ' + str(from_node_lat) + ', ' + \\\n",
        "                   str(to_node_lon) + ' ' + str(to_node_lat) + ')'\n",
        "        agency_name = row.agency_name\n",
        "        allowed_use = _allowed_use_function(row.route_type)\n",
        "\n",
        "        # inbound links (boarding)\n",
        "\n",
        "        VDF_fftt1 = \\\n",
        "            0.5 * ((period_end_time - period_start_time) / frequency_dict[row.directed_service_id])\n",
        "        VDF_fftt1 = min(VDF_fftt1, 10)\n",
        "        # waiting time at a station is 10 minutes at most\n",
        "        geometry = 'LINESTRING (' + str(to_node_lon) + ' ' + str(to_node_lat) + ', ' + \\\n",
        "                   str(from_node_lon) + ' ' + str(from_node_lat) + ')'\n",
        "        # inbound link is average waiting time derived from frequency\n",
        "        link_list_inbound = [link_id, from_node_id, to_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                             link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                             VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use,\n",
        "                             agency_name,\n",
        "                             stop_sequence, directed_service_id]\n",
        "        number_of_sta2route_links += 1\n",
        "\n",
        "        # outbound links (boarding)\n",
        "        link_id = agency_num * 1000000 + number_of_route_links + number_of_sta2route_links\n",
        "        VDF_fftt1 = 1  # (length / free_speed) * 60\n",
        "        #  the time of outbound time\n",
        "        link_list_outbound = [link_id, to_node_id, from_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                              link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                              VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use,\n",
        "                              agency_name,\n",
        "                              stop_sequence, directed_service_id]\n",
        "        one_agency_link_list.append(link_list_inbound)\n",
        "        one_agency_link_list.append(link_list_outbound)\n",
        "        number_of_sta2route_links += 1\n",
        "        #  one inbound link and one outbound link\n",
        "        if number_of_sta2route_links % 50 == 0:\n",
        "            time_end = time.time()\n",
        "            print('convert ', number_of_sta2route_links,\n",
        "                  'boarding links successfully...', 'using time', time_end - time_start, 's')\n",
        "\n",
        "    return one_agency_link_list\n",
        "\n",
        "\n",
        "def create_transferring_links(all_node_df, all_link_list):\n",
        "    physical_node_df = all_node_df[all_node_df.node_id == all_node_df.physical_node_id]\n",
        "    physical_node_df = physical_node_df.reset_index()\n",
        "    number_of_transferring_links = 0\n",
        "    time_start = time.time()\n",
        "    for i in range(len(physical_node_df)):\n",
        "        ref_x = physical_node_df.iloc[i].x_coord\n",
        "        ref_y = physical_node_df.iloc[i].y_coord\n",
        "        neighboring_node_df = physical_node_df[(physical_node_df.x_coord >= (ref_x - 0.003)) &\n",
        "                                               (physical_node_df.x_coord <= (ref_x + 0.003))]\n",
        "        neighboring_node_df = neighboring_node_df[(neighboring_node_df.y_coord >= (ref_y - 0.003)) &\n",
        "                                                  (neighboring_node_df.y_coord <= (ref_y + 0.003))]\n",
        "        labeled_list = []\n",
        "        count = 0\n",
        "        for j in range(len(neighboring_node_df)):\n",
        "            if count >= 10:\n",
        "                break\n",
        "            if (physical_node_df.iloc[i].route_id, physical_node_df.iloc[i].agency_name) == \\\n",
        "                    (neighboring_node_df.iloc[j].route_id, neighboring_node_df.iloc[j].agency_name):\n",
        "                continue\n",
        "            from_node_lon = float(physical_node_df.iloc[i].x_coord)\n",
        "            from_node_lat = float(physical_node_df.iloc[i].y_coord)\n",
        "            to_node_lon = float(neighboring_node_df.iloc[j].x_coord)\n",
        "            to_node_lat = float(neighboring_node_df.iloc[j].y_coord)\n",
        "            length = _calculate_distance_from_geometry(from_node_lon, from_node_lat, to_node_lon, to_node_lat)\n",
        "            if (length > 321.869) | (length < 1):\n",
        "                continue\n",
        "            if (neighboring_node_df.iloc[j].route_id, neighboring_node_df.iloc[j].agency_name) in labeled_list:\n",
        "                continue\n",
        "            count += 1\n",
        "            labeled_list.append((neighboring_node_df.iloc[j].route_id, neighboring_node_df.iloc[j].agency_name))\n",
        "            # consider only one stops of another route\n",
        "            # transferring 1\n",
        "            #  print('transferring link length =', length)\n",
        "            link_id = number_of_transferring_links + 1\n",
        "            from_node_id = physical_node_df.iloc[i].node_id\n",
        "            to_node_id = neighboring_node_df.iloc[j].node_id\n",
        "            facility_type = 'sta2sta'\n",
        "            dir_flag = 1\n",
        "            directed_route_id = -1\n",
        "            link_type = 3\n",
        "            link_type_name = 'transferring_links'\n",
        "            lanes = 1\n",
        "            capacity = 999999\n",
        "            VDF_fftt1 = (length / 1000) / 1\n",
        "            VDF_cap1 = lanes * capacity\n",
        "            free_speed = 1\n",
        "            # 1 kilo/hour\n",
        "            VDF_alpha1 = 0.15\n",
        "            VDF_beta1 = 4\n",
        "            VDF_penalty1 = _transferring_penalty(physical_node_df.iloc[i].node_type, neighboring_node_df.iloc[j].node_type)\n",
        "            # penalty of transferring\n",
        "            cost = 60\n",
        "            geometry = 'LINESTRING (' + str(from_node_lon) + ' ' + str(from_node_lat) + ', ' + \\\n",
        "                       str(to_node_lon) + ' ' + str(to_node_lat) + ')'\n",
        "            agency_name = \"\"\n",
        "            allowed_use = \\\n",
        "                _allowed_use_transferring(physical_node_df.iloc[i].node_type, neighboring_node_df.iloc[j].node_type)\n",
        "            stop_sequence = \"\"\n",
        "            directed_service_id = \"\"\n",
        "            link_list = [link_id, from_node_id, to_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                         link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                         VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use, agency_name,\n",
        "                         stop_sequence, directed_service_id]\n",
        "            all_link_list.append(link_list)\n",
        "            # transferring 2\n",
        "            number_of_transferring_links += 1\n",
        "            geometry = 'LINESTRING (' + str(to_node_lon) + ' ' + str(to_node_lat) + ', ' + \\\n",
        "                       str(from_node_lon) + ' ' + str(from_node_lat) + ')'\n",
        "            link_id = number_of_transferring_links + 1\n",
        "            link_list = [link_id, to_node_id, from_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                         link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                         VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use, agency_name,\n",
        "                         stop_sequence, directed_service_id]\n",
        "            all_link_list.append(link_list)\n",
        "            number_of_transferring_links += 1\n",
        "            if number_of_transferring_links % 50 == 0:\n",
        "                time_end = time.time()\n",
        "                print('convert ', number_of_transferring_links,\n",
        "                      'transferring links successfully...', 'using time', time_end - time_start, 's')\n",
        "\n",
        "    return all_link_list\n",
        "\n",
        "\n",
        "\"\"\" ------------------functions------------------ \"\"\"\n",
        "\n",
        "\n",
        "def _stop_sequence_label(trip_stop_time_df):\n",
        "    trip_stop_time_df = trip_stop_time_df.sort_values(by=['stop_sequence'])\n",
        "    trip_stop_time_df['stop_sequence_label'] = ';'.join(np.array(trip_stop_time_df.stop_sequence).astype(str))\n",
        "    return trip_stop_time_df\n",
        "\n",
        "\n",
        "def _reading_text(filename):\n",
        "    file_path = filename + '.txt'\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
        "        lines = f.readlines()\n",
        "        first_line = lines[0].split('\\n')[0].split(',')\n",
        "        for line in lines:\n",
        "            if len(line.split('\\n')[0].split(',')) == len(first_line):\n",
        "                data.append(line.split('\\n')[0].split(','))\n",
        "            else:\n",
        "                data.append(_split_ignore_separators_in_quoted(line))\n",
        "    data_frame = pd.DataFrame(data[1:], columns=data[0])\n",
        "    return data_frame\n",
        "\n",
        "\n",
        "def _determine_terminal_flag(trip_stop_time_df):\n",
        "    trip_stop_time_df.stop_sequence = trip_stop_time_df.stop_sequence.astype('int32')\n",
        "    start_stop_seq = int(trip_stop_time_df.stop_sequence.min())\n",
        "    end_stop_seq = int(trip_stop_time_df.stop_sequence.max())\n",
        "    #  convert string to integer\n",
        "    trip_stop_time_df['terminal_flag'] = \\\n",
        "        ((trip_stop_time_df.stop_sequence == start_stop_seq) |\n",
        "         (trip_stop_time_df.stop_sequence == end_stop_seq)).astype('int32')\n",
        "    return trip_stop_time_df\n",
        "\n",
        "\n",
        "def _allowed_use_function(route_type):\n",
        "    #  convert route type to node type on service network\n",
        "    allowed_use = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        allowed_use = \"w_bus_only;w_bus_metro;d_bus_only;d_bus_metro\"\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        allowed_use = \"w_metro_only;w_bus_metro;d_metro_only;d_bus_metro\"\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        allowed_use = \"w_rail_only;d_rail_only\"\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        allowed_use = \"w_bus_only;w_bus_metro;d_bus_only;d_bus_metro\"\n",
        "    return allowed_use\n",
        "\n",
        "\n",
        "def _allowed_use_transferring(node_type_1, node_type_2):\n",
        "    if (node_type_1 == 'stop') & (node_type_2 == 'stop'):\n",
        "        allowed_use = \"w_bus_only;d_bus_only\"\n",
        "    elif (node_type_1 == 'stop') & (node_type_2 == 'metro_station'):\n",
        "        allowed_use = \"w_bus_metro;d_bus_metro\"\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'stop'):\n",
        "        allowed_use = \"w_bus_metro;d_bus_metro\"\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'metro_station'):\n",
        "        allowed_use = \"w_metro_only;d_metro_only\"\n",
        "    elif (node_type_1 == 'rail_station') & (node_type_2 == 'rail_station'):\n",
        "        allowed_use = \"w_rail_only;d_rail_only\"\n",
        "    else:\n",
        "        allowed_use = \"closed\"\n",
        "\n",
        "    return allowed_use\n",
        "\n",
        "\n",
        "def _transferring_penalty(node_type_1, node_type_2):\n",
        "    if (node_type_1 == 'stop') & (node_type_2 == 'stop'):\n",
        "        VDF_penalty1 = 99\n",
        "    elif (node_type_1 == 'stop') & (node_type_2 == 'metro_station'):\n",
        "        VDF_penalty1 = 0\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'stop'):\n",
        "        VDF_penalty1 = 0\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'metro_station'):\n",
        "        VDF_penalty1 = 99\n",
        "    elif (node_type_1 == 'rail_station') & (node_type_2 == 'rail_station'):\n",
        "        VDF_penalty1 = 99\n",
        "    else:\n",
        "        VDF_penalty1 = 1000\n",
        "\n",
        "    return VDF_penalty1\n",
        "\n",
        "\n",
        "def _convert_route_type_to_node_type_p(route_type):\n",
        "    #  convert route type to node type on physical network\n",
        "    node_type = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        node_type = 'stop'\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        node_type = 'metro_station'\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        node_type = 'rail_station'\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        node_type = 'stop'\n",
        "    return node_type\n",
        "\n",
        "\n",
        "def _convert_route_type_to_node_type_s(route_type):\n",
        "    #  convert route type to node type on service network\n",
        "    node_type = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        node_type = 'tram_service_node'\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        node_type = 'metro_service_node'\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        node_type = 'rail_service_node'\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        node_type = 'bus_service_node'\n",
        "    return node_type\n",
        "\n",
        "\n",
        "def _convert_route_type_to_link_type(route_type):\n",
        "    #  convert route type to node type on service network\n",
        "    link_type = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        link_type = 'tram'\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        link_type = 'metro'\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        link_type = 'rail'\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        link_type = 'bus'\n",
        "    return link_type\n",
        "\n",
        "\n",
        "def _split_ignore_separators_in_quoted(s, separator=',', quote_mark='\"'):\n",
        "    result = []\n",
        "    quoted = False\n",
        "    current = ''\n",
        "    for i in range(len(s)):\n",
        "        if quoted:\n",
        "            current += s[i]\n",
        "            if s[i] == quote_mark:\n",
        "                quoted = False\n",
        "            continue\n",
        "        if s[i] == separator:\n",
        "            result.append(current.strip())\n",
        "            current = ''\n",
        "        else:\n",
        "            current += s[i]\n",
        "            if s[i] == quote_mark:\n",
        "                quoted = True\n",
        "    result.append(current)\n",
        "    return result\n",
        "\n",
        "\n",
        "def _calculate_distance_from_geometry(lon1, lat1, lon2, lat2):  # WGS84 transfer coordinate system to distance(mile) #xy\n",
        "    radius = 6371\n",
        "    d_latitude = (lat2 - lat1) * math.pi / 180.0\n",
        "    d_longitude = (lon2 - lon1) * math.pi / 180.0\n",
        "\n",
        "    a = math.sin(d_latitude / 2) * math.sin(d_latitude / 2) + math.cos(lat1 * math.pi / 180.0) * math.cos(\n",
        "        lat2 * math.pi / 180.0) * math.sin(d_longitude / 2) * math.sin(d_longitude / 2)\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "    # distance = radius * c * 1000 / 1609.34  # mile\n",
        "    distance = radius * c * 1000  # meter\n",
        "    return distance\n",
        "\n",
        "\n",
        "def _hhmm_to_minutes(time_period_1):\n",
        "    from_time_1 = datetime.time(int(time_period_1[0:2]), int(time_period_1[2:4]))\n",
        "    to_time_1 = datetime.time(int(time_period_1[-4:-2]), int(time_period_1[-2:]))\n",
        "    from_time_min_1 = from_time_1.hour * 60 + from_time_1.minute\n",
        "    to_time_min_1 = to_time_1.hour * 60 + to_time_1.minute\n",
        "    return from_time_min_1, to_time_min_1\n",
        "\n",
        "\n",
        "\"\"\" ------------------main functions------------------ \"\"\"\n",
        "\n",
        "\n",
        "def gtfs2gmns(input_path, output_path):\n",
        "    start_time = time.time()\n",
        "    folders = [folder for folder in os.listdir(input_path) if \"check\" not in folder]\n",
        "    gtfs_folder_list = []\n",
        "    for sub_folder in folders:\n",
        "        sub_folder_path = input_path + '/' + sub_folder\n",
        "        if os.path.isdir(sub_folder_path):  # check whether the specified path is an existing directory or not.\n",
        "            gtfs_folder_list.append(sub_folder_path)\n",
        "    if len(gtfs_folder_list) == 0:\n",
        "        gtfs_folder_list.append(input_path)\n",
        "\n",
        "    all_node_list = []\n",
        "    all_link_list = []\n",
        "    for i in range(len(gtfs_folder_list)):\n",
        "        print('Start converting Agency_{}...'.format(i + 1))\n",
        "        print('Directory : ' + str(gtfs_folder_list[i]))\n",
        "        agency_gtfs_path = gtfs_folder_list[i]\n",
        "        \"\"\" step 1. reading data \"\"\"\n",
        "        stop_df, route_df, trip_df, trip_route_df, stop_time_df, directed_trip_route_stop_time_df = \\\n",
        "            reading_data(agency_gtfs_path)\n",
        "        #  directed_trip_route_stop_time_df.to_csv(gtfs_folder_list[i] + '/timetable.csv', index=False)\n",
        "        #  directed_trip_route_stop_time_df = pd.read_csv(gtfs_folder_list[i] + '/timetable.csv')\n",
        "\n",
        "        \"\"\"step 2. create nodes\"\"\"\n",
        "        agency_num = i + 1\n",
        "        # number of agency equals to i+1\n",
        "        node_df = create_nodes(directed_trip_route_stop_time_df, agency_num)\n",
        "        all_node_list.append(node_df)\n",
        "        print(\"node.csv of\", str(gtfs_folder_list[i]), \"has been generated...\")\n",
        "        \"\"\"step 3. create links\"\"\"\n",
        "        all_link_list \\\n",
        "            = create_service_boarding_links(directed_trip_route_stop_time_df, node_df, agency_num, all_link_list)\n",
        "\n",
        "        if i == len(gtfs_folder_list):\n",
        "            print('output')\n",
        "        print('Conversion of  Agency{}...'.format(agency_num + 1), ' have done..')\n",
        "\n",
        "    all_node_df = pd.concat(all_node_list)\n",
        "    all_node_df.reset_index(inplace=True)\n",
        "    all_node_df = all_node_df.drop(['index'], axis=1)\n",
        "    # transferring links\n",
        "    all_link_list = create_transferring_links(all_node_df, all_link_list)\n",
        "\n",
        "    all_link_df = pd.DataFrame(all_link_list)\n",
        "    all_link_df.rename(columns={0: 'link_id',\n",
        "                                1: 'from_node_id',\n",
        "                                2: 'to_node_id',\n",
        "                                3: 'facility_type',\n",
        "                                4: 'dir_flag',\n",
        "                                5: 'directed_route_id',\n",
        "                                6: 'link_type',\n",
        "                                7: 'link_type_name',\n",
        "                                8: 'length',\n",
        "                                9: 'lanes',\n",
        "                                10: 'capacity',\n",
        "                                11: 'free_speed',\n",
        "                                12: 'cost',\n",
        "                                13: 'VDF_fftt1',\n",
        "                                14: 'VDF_cap1',\n",
        "                                15: 'VDF_alpha1',\n",
        "                                16: 'VDF_beta1',\n",
        "                                17: 'VDF_penalty1',\n",
        "                                18: 'geometry',\n",
        "                                19: 'VDF_allowed_uses1',\n",
        "                                20: 'agency_name',\n",
        "                                21: 'stop_sequence',\n",
        "                                22: 'directed_service_id'}, inplace=True)\n",
        "    all_node_df.to_csv('node.csv', index=False)\n",
        "    #  zone_df = pd.read_csv('zone.csv')\n",
        "    #  source_node_df = pd.read_csv('source_node.csv')\n",
        "    #  node_df = pd.concat([zone_df, all_node_df])\n",
        "    #  node_df.to_csv(\"node.csv\", index=False)\n",
        "    all_link_df = all_link_df.drop_duplicates(\n",
        "        subset=['from_node_id', 'to_node_id'],\n",
        "        keep='last').reset_index(drop=True)\n",
        "    all_link_df.to_csv('link.csv', index=False)\n",
        "    print('run time -->', time.time() - start_time)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    global period_start_time\n",
        "    global period_end_time\n",
        "    input_path = '/Users/willicon/Desktop/roanoke_benchmark/gtfs'\n",
        "    output_path = '/Users/willicon/Desktop/roanoke_benchmark/89780'\n",
        "    time_period_id = 1\n",
        "    time_period = '0000_2359'\n",
        "    period_start_time, period_end_time = _hhmm_to_minutes(time_period)\n",
        "\n",
        "    gtfs2gmns(input_path, output_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mw8Jzv2ikXI"
      },
      "source": [
        "##Download GMNS data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mDf0i_KS7dIH",
        "outputId": "0a2dac3d-6bff-432f-d9df-ca4b70377e37"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_2eaf2c61-077f-4b27-9d48-4af32d120b91\", \"node.csv\", 4636)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/GTFS2GMNS/test/node.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OK7TwXb0pZNf",
        "outputId": "ce3a5b50-2ca0-4692-c8fb-70240665b0a3"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_35847aa2-5cce-45b4-8d5a-c3ed43b0705e\", \"link.csv\", 11705)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/GTFS2GMNS/test/link.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6pyZEum8Ood"
      },
      "source": [
        "**Visualization using GMNS tool:**\n",
        "By simply uploading node.csv and link.csv at https://asu-trans-ai-lab.github.io/index.html#,  \n",
        " you can easily create custom online maps for any GMNS network files. \n",
        "To view zone and demand information please visit this page to use QGIS/NeXTA tools. https://github.com/asu-trans-ai-lab/traffic-engineering-and-analysis/blob/master/undergraduate_student_project/QGIS%20For%20Gmns%20User%20Guide_v0.5.pdf "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
